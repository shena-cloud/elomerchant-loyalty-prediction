{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "import prettytable\n",
    "import lightgbm as lgbm\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import datetime\n",
    "import gc\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/hiralmshah/reduce-memory-usage-trick-with-elo-merchant-data\n",
    "def reduce_memory(dataframe, verbose =True):\n",
    "    numerical = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    initial_memory = dataframe.memory_usage().sum()/1024**2\n",
    "    for col in dataframe.columns:\n",
    "        col_type = dataframe[col].dtypes\n",
    "        if col_type == object:\n",
    "            dataframe[col] = dataframe[col].astype('category')\n",
    "        if col_type in numerical:\n",
    "            c_min = dataframe[col].min()\n",
    "            c_max = dataframe[col].max()\n",
    "            if str(col_type)[:3] =='int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float64).min and c_max < np.finfo(np.float64).max:\n",
    "                    dataframe[col] = dataframe[col].astype(np.float64)\n",
    "    final_memory = dataframe.memory_usage().sum()/ 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage reduced to {:5.2f} Mb({:.1f}% reduction)'.format(final_memory, (initial_memory-final_memory)/initial_memory*100))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_featurization(train, test):\n",
    "    #imputing missing value with mode\n",
    "    test['first_active_month'].fillna(test['first_active_month'].mode()[0], inplace = True)\n",
    "    #converting object datatype to datetime to extract more features\n",
    "    train['first_active_month'] = pd.to_datetime(train['first_active_month'])\n",
    "    test['first_active_month'] = pd.to_datetime(test['first_active_month'])\n",
    "    #creating a new feature outlier based on loyalty score\n",
    "    train['outliers'] = 0\n",
    "    train.loc[train['target'] < -30, 'outliers'] =1\n",
    "    #mean encoding categorical features\n",
    "    for feature in ['feature_1', 'feature_2', 'feature_3']:\n",
    "        mean_value = train.groupby(feature)['target'].mean()\n",
    "        train[feature] = train[feature].map(mean_value)\n",
    "        test[feature] = test[feature].map(mean_value)\n",
    "    train['date_elapsed'] = (datetime.datetime.today() - train['first_active_month']).dt.days\n",
    "    train['first_month'] = train['first_active_month'].dt.month\n",
    "    train['first_year'] = train['first_active_month'].dt.year\n",
    "    train['first_week'] = train['first_active_month'].dt.dayofweek\n",
    "    train['first_quarter'] = train['first_active_month'].dt.quarter\n",
    "    train['date_elapsed_feature_1'] = train['date_elapsed'] * train['feature_1']\n",
    "    train['date_elapsed_feature_2'] = train['date_elapsed'] * train['feature_2']\n",
    "    train['date_elapsed_feature_3'] = train['date_elapsed'] * train['feature_3']\n",
    "\n",
    "    train['date_elapsed_feature_1_ratio'] = train['date_elapsed'] / train['feature_1']\n",
    "    train['date_elapsed_feature_2_ratio'] = train['date_elapsed'] / train['feature_2']\n",
    "    train['date_elapsed_feature_3_ratio'] = train['date_elapsed'] / train['feature_3']\n",
    "\n",
    "    train['feature_sum'] = train['feature_1'] + train['feature_2'] + train['feature_3']\n",
    "    train['feature_mean'] = train[['feature_1', 'feature_2', 'feature_3']].mean(axis = 1)\n",
    "    train['feature_max'] = train[['feature_1', 'feature_2', 'feature_3']].max(axis = 1)\n",
    "    train['feature_min'] = train[['feature_1', 'feature_2', 'feature_3']].min(axis = 1)\n",
    "    train['feature_var'] = train[['feature_1', 'feature_2', 'feature_3']].std(axis = 1)\n",
    "    \n",
    "    test['date_elapsed'] = (datetime.datetime.today() - test['first_active_month']).dt.days\n",
    "    test['first_month'] = test['first_active_month'].dt.month\n",
    "    test['first_year'] = test['first_active_month'].dt.year\n",
    "    test['first_week'] = test['first_active_month'].dt.dayofweek\n",
    "    test['first_quarter'] = test['first_active_month'].dt.quarter\n",
    "    \n",
    "    test['date_elapsed_feature_1'] = test['date_elapsed'] * test['feature_1']\n",
    "    test['date_elapsed_feature_2'] = test['date_elapsed'] * test['feature_2']\n",
    "    test['date_elapsed_feature_3'] = test['date_elapsed'] * test['feature_3']\n",
    "\n",
    "    test['date_elapsed_feature_1_ratio'] = test['date_elapsed'] / test['feature_1']\n",
    "    test['date_elapsed_feature_2_ratio'] = test['date_elapsed'] / test['feature_2']\n",
    "    test['date_elapsed_feature_3_ratio'] = test['date_elapsed'] / test['feature_3']\n",
    "\n",
    "    test['feature_sum'] = test['feature_1'] + test['feature_2'] + test['feature_3']\n",
    "    test['feature_mean'] = test[['feature_1', 'feature_2', 'feature_3']].mean(axis = 1)\n",
    "    test['feature_max'] = test[['feature_1', 'feature_2', 'feature_3']].max(axis = 1)\n",
    "    test['feature_min'] = test[['feature_1', 'feature_2', 'feature_3']].min(axis = 1)\n",
    "    test['feature_var'] = test[['feature_1', 'feature_2', 'feature_3']].std(axis = 1)\n",
    "    \n",
    "    gc.collect()\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historical_transactions_featurization(historical_transactions):\n",
    "    # imputing missing values in historical_transactions\n",
    "    historical_transactions['category_3'].fillna(historical_transactions['category_3'].mode()[0], inplace = True)\n",
    "    historical_transactions['merchant_id'].fillna(historical_transactions['merchant_id'].mode()[0], inplace = True)\n",
    "    historical_transactions['category_2'].fillna(historical_transactions['category_2'].mode()[0], inplace = True)\n",
    "    \n",
    "    #encoding categorical features\n",
    "    historical_transactions['category_1'] = historical_transactions['category_1'].map({'Y':1, 'N':0}).astype(int)\n",
    "    historical_transactions['category_3'] =historical_transactions['category_3'].map({'A':0, 'B':1, 'C':2}).astype(int)\n",
    "    historical_transactions['authorized_flag'] = historical_transactions['authorized_flag'].map({'Y':1, 'N':0}).astype(int)\n",
    "    \n",
    "    historical_transactions['installments'].replace(-1, np.nan, inplace = True)\n",
    "    historical_transactions['installments'].replace(999, np.nan, inplace = True)\n",
    "    historical_transactions['installments'].fillna(historical_transactions['installments'].mode()[0], inplace = True)\n",
    "    \n",
    "    historical_transactions['purchase_date'] = pd.to_datetime(historical_transactions['purchase_date'])\n",
    "    historical_transactions['purchase_year'] = historical_transactions['purchase_date'].dt.year\n",
    "    historical_transactions['purchase_day'] = historical_transactions['purchase_date'].dt.day\n",
    "    historical_transactions['purchase_month'] = historical_transactions['purchase_date'].dt.month\n",
    "    historical_transactions['purchase_week'] = historical_transactions['purchase_date'].dt.week\n",
    "    historical_transactions['purchase_day'] = historical_transactions['purchase_date'].dt.day\n",
    "    historical_transactions['purchase_dayofweek'] = historical_transactions['purchase_date'].dt.dayofweek\n",
    "    historical_transactions['purchase_dayofyear'] = historical_transactions['purchase_date'].dt.dayofyear\n",
    "    historical_transactions['purchase_hour'] = historical_transactions['purchase_date'].dt.hour\n",
    "    historical_transactions['purchase_minute'] = historical_transactions['purchase_date'].dt.minute\n",
    "    historical_transactions['purchase_second'] = historical_transactions['purchase_date'].dt.second\n",
    "    historical_transactions['purchase_month_end'] = historical_transactions['purchase_date'].dt.is_month_end.astype(int)\n",
    "    historical_transactions['purchase_month_start'] = historical_transactions['purchase_date'].dt.is_month_start.astype(int)\n",
    "    historical_transactions['purchase_quarter_start'] = historical_transactions['purchase_date'].dt.is_quarter_start.astype(int)\n",
    "    historical_transactions['purchase_quarter_end'] = historical_transactions['purchase_date'].dt.is_quarter_end.astype(int)\n",
    "    historical_transactions['purchase_year_start'] = historical_transactions['purchase_date'].dt.is_year_start.astype(int)\n",
    "    historical_transactions['purchase_year_end'] = historical_transactions['purchase_date'].dt.is_year_end.astype(int)\n",
    "    \n",
    "    historical_transactions['purchase_is_weekend'] = (historical_transactions.purchase_dayofweek >=5).astype(int)\n",
    "    historical_transactions['purchase_is_weekday'] = (historical_transactions.purchase_dayofweek <5).astype(int)\n",
    "    historical_transactions['month_difference'] = ((datetime.datetime.today() - historical_transactions['purchase_date']).dt.days)//30\n",
    "    historical_transactions['month_difference'] += historical_transactions['month_lag']\n",
    "    #denormalizing purchase amount\n",
    "    historical_transactions['purchase_amount'] = np.round(historical_transactions['purchase_amount']/0.00150265118 + 497.06 , 2)\n",
    "    \n",
    "    historical_transactions = reduce_memory(historical_transactions)\n",
    "    \n",
    "    #aggregation with grouping by card id\n",
    "    aggregation_dict = {'card_id' : ['size'],\n",
    "                    'city_id': ['nunique'],\n",
    "                   'category_1': ['max', 'min', 'sum', 'mean'],\n",
    "                   'installments' : ['sum', 'max', 'min', 'mean', 'var','skew'],\n",
    "                   'category_3' : ['sum', 'mean'],\n",
    "                   'merchant_category_id': ['nunique'],\n",
    "                   'merchant_id': ['nunique'],\n",
    "                   'month_lag': ['sum', 'max', 'min', 'var', 'mean', 'skew'],\n",
    "                   'purchase_amount' : ['sum', 'mean', 'var', 'max', 'min', 'skew'],\n",
    "                   'purchase_date' : ['max', 'min'],\n",
    "                   'category_2': ['sum', 'mean'],\n",
    "                   'state_id' : ['nunique'],\n",
    "                   'subsector_id' : ['nunique'],\n",
    "                   'purchase_year' : ['max', 'min', 'nunique'],\n",
    "                   'purchase_day' : ['max', 'min', 'nunique'],\n",
    "                   'purchase_month' : ['max', 'min', 'nunique'],\n",
    "                   'purchase_week' : ['max', 'min', 'nunique'],\n",
    "                   'purchase_dayofweek' : ['max', 'min', 'nunique','mean'],\n",
    "                   'purchase_dayofyear' : ['max', 'min', 'nunique', 'mean'],\n",
    "                   'purchase_hour' : ['max', 'min', 'nunique', 'mean'],\n",
    "                   'purchase_minute': ['max', 'min', 'nunique', 'mean'],\n",
    "                   'purchase_second': ['max', 'min', 'nunique', 'mean'],\n",
    "                   'purchase_month_end' : ['nunique', 'mean'],\n",
    "                   'purchase_month_start' : ['nunique', 'mean'],\n",
    "                   'purchase_quarter_start' : ['nunique', 'mean'],\n",
    "                   'purchase_quarter_end' : ['nunique', 'mean'],\n",
    "                   'purchase_year_start': ['nunique', 'mean'],\n",
    "                   'purchase_year_end': ['nunique', 'mean'],\n",
    "                   'purchase_is_weekday' : ['nunique', 'mean'],\n",
    "                   'purchase_is_weekend' : ['nunique', 'mean'],\n",
    "                   'month_difference' : ['max', 'min', 'mean', 'var', 'skew']}\n",
    "    historical_transactions_aggregated = historical_transactions.groupby('card_id').agg(aggregation_dict)\n",
    "    historical_transactions_aggregated.columns = ['trans_' + '_'.join(col) for col in historical_transactions_aggregated.columns.values]\n",
    "    historical_transactions_aggregated.reset_index(inplace = True)\n",
    "    \n",
    "    historical_transactions_aggregated['trans_purchase_max_min'] = (historical_transactions_aggregated['trans_purchase_date_max'] - historical_transactions_aggregated['trans_purchase_date_min']).dt.days\n",
    "    historical_transactions_aggregated['trans_purchase_date_uptomax'] = (datetime.datetime.today() - historical_transactions_aggregated['trans_purchase_date_max']).dt.days\n",
    "    historical_transactions_aggregated['trans_purchase_date_uptomin'] = (datetime.datetime.today() - historical_transactions_aggregated['trans_purchase_date_min']).dt.days\n",
    "    \n",
    "    gc.collect()\n",
    "    return historical_transactions_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_merchants_featurization(new_merchant_trans):\n",
    "    # imputing missing values in new_merchant_transactions\n",
    "    new_merchant_trans['category_3'].fillna(new_merchant_trans['category_3'].mode()[0], inplace = True)\n",
    "    new_merchant_trans['merchant_id'].fillna(new_merchant_trans['merchant_id'].mode()[0], inplace = True)\n",
    "    new_merchant_trans['category_2'].fillna(new_merchant_trans['category_2'].mode()[0], inplace = True)\n",
    "\n",
    "    #encoding categorical features\n",
    "    new_merchant_trans['category_1'] = new_merchant_trans['category_1'].map({'Y':1, 'N':0}).astype(int)\n",
    "    new_merchant_trans['category_3'] = new_merchant_trans['category_3'].map({'A':0, 'B':1, 'C':2}).astype(int)\n",
    "    new_merchant_trans['authorized_flag'] = new_merchant_trans['authorized_flag'].map({'Y':1, 'N':0}).astype(int)\n",
    "    \n",
    "    new_merchant_trans['installments'].replace(-1, np.nan, inplace = True)\n",
    "    new_merchant_trans['installments'].replace(999, np.nan, inplace = True)\n",
    "    new_merchant_trans['installments'].fillna(new_merchant_trans['installments'].mode()[0], inplace = True)\n",
    "    \n",
    "    new_merchant_trans['purchase_date'] = pd.to_datetime(new_merchant_trans['purchase_date'])\n",
    "    new_merchant_trans['purchase_year'] = new_merchant_trans['purchase_date'].dt.year\n",
    "    new_merchant_trans['purchase_day'] = new_merchant_trans['purchase_date'].dt.day\n",
    "    new_merchant_trans['purchase_month'] = new_merchant_trans['purchase_date'].dt.month\n",
    "    new_merchant_trans['purchase_week'] = new_merchant_trans['purchase_date'].dt.week\n",
    "    new_merchant_trans['purchase_day'] = new_merchant_trans['purchase_date'].dt.day\n",
    "    new_merchant_trans['purchase_dayofweek'] = new_merchant_trans['purchase_date'].dt.dayofweek\n",
    "    new_merchant_trans['purchase_dayofyear'] = new_merchant_trans['purchase_date'].dt.dayofyear\n",
    "    new_merchant_trans['purchase_hour'] = new_merchant_trans['purchase_date'].dt.hour\n",
    "    new_merchant_trans['purchase_minute'] = new_merchant_trans['purchase_date'].dt.minute\n",
    "    new_merchant_trans['purchase_second'] = new_merchant_trans['purchase_date'].dt.second\n",
    "    new_merchant_trans['purchase_month_end'] = new_merchant_trans['purchase_date'].dt.is_month_end.astype(int)\n",
    "    new_merchant_trans['purchase_month_start'] = new_merchant_trans['purchase_date'].dt.is_month_start.astype(int)\n",
    "    new_merchant_trans['purchase_quarter_start'] = new_merchant_trans['purchase_date'].dt.is_quarter_start.astype(int)\n",
    "    new_merchant_trans['purchase_quarter_end'] =new_merchant_trans['purchase_date'].dt.is_quarter_end.astype(int)\n",
    "    new_merchant_trans['purchase_year_start'] = new_merchant_trans['purchase_date'].dt.is_year_start.astype(int)\n",
    "    new_merchant_trans['purchase_year_end'] = new_merchant_trans['purchase_date'].dt.is_year_end.astype(int)\n",
    "    \n",
    "    new_merchant_trans['purchase_is_weekend'] = (new_merchant_trans.purchase_dayofweek >=5).astype(int)\n",
    "    new_merchant_trans['purchase_is_weekday'] = (new_merchant_trans.purchase_dayofweek <5).astype(int)\n",
    "    new_merchant_trans['month_difference'] = ((datetime.datetime.today() - new_merchant_trans['purchase_date']).dt.days)//30\n",
    "    new_merchant_trans['month_difference'] += new_merchant_trans['month_lag']\n",
    "    # denormalizing purchase amount\n",
    "    new_merchant_trans['purchase_amount'] = np.round(new_merchant_trans['purchase_amount']/0.00150265118 + 497.06, 2)\n",
    "    \n",
    "    new_merchant_trans = reduce_memory(new_merchant_trans)\n",
    "    \n",
    "    #aggregating historical_transactions by card_id\n",
    "    aggregation_dict = {'card_id' : ['size'],\n",
    "                    'city_id': ['nunique'],\n",
    "                   'category_1': ['max', 'min', 'sum', 'mean'],\n",
    "                   'installments' : ['sum', 'max', 'min', 'mean', 'var','skew'],\n",
    "                   'category_3' : ['sum', 'mean'],\n",
    "                   'merchant_category_id': ['nunique'],\n",
    "                   'merchant_id': ['nunique'],\n",
    "                   'month_lag': ['sum', 'max', 'min', 'var', 'mean', 'skew'],\n",
    "                   'purchase_amount' : ['sum', 'mean', 'var', 'max', 'min', 'skew'],\n",
    "                   'purchase_date' : ['max', 'min'],\n",
    "                   'category_2': ['sum', 'mean'],\n",
    "                   'state_id' : ['nunique'],\n",
    "                   'subsector_id' : ['nunique'],\n",
    "                   'purchase_year' : ['max', 'min', 'nunique'],\n",
    "                   'purchase_day' : ['max', 'min', 'nunique'],\n",
    "                   'purchase_month' : ['max', 'min', 'nunique'],\n",
    "                   'purchase_week' : ['max', 'min', 'nunique'],\n",
    "                   'purchase_dayofweek' : ['max', 'min', 'nunique','mean'],\n",
    "                   'purchase_dayofyear' : ['max', 'min', 'nunique', 'mean'],\n",
    "                   'purchase_hour' : ['max', 'min', 'nunique', 'mean'],\n",
    "                   'purchase_minute': ['max', 'min', 'nunique', 'mean'],\n",
    "                   'purchase_second': ['max', 'min', 'nunique', 'mean'],\n",
    "                   'purchase_month_end' : ['nunique', 'mean'],\n",
    "                   'purchase_month_start' : ['nunique', 'mean'],\n",
    "                   'purchase_quarter_start' : ['nunique', 'mean'],\n",
    "                   'purchase_quarter_end' : ['nunique', 'mean'],\n",
    "                   'purchase_year_start': ['nunique', 'mean'],\n",
    "                   'purchase_year_end': ['nunique', 'mean'],\n",
    "                   'purchase_is_weekday' : ['nunique', 'mean'],\n",
    "                   'purchase_is_weekend' : ['nunique', 'mean'],\n",
    "                   'month_difference' : ['max', 'min', 'mean', 'var', 'skew']}\n",
    "    new_merchant_trans_aggregated =  new_merchant_trans.groupby('card_id').agg(aggregation_dict)\n",
    "    new_merchant_trans_aggregated.columns = ['new_' + '_'.join(col) for col in new_merchant_trans_aggregated.columns.values]\n",
    "    new_merchant_trans_aggregated.reset_index(inplace = True)\n",
    "    new_merchant_trans_aggregated['new_purchase_max_min'] = (new_merchant_trans_aggregated['new_purchase_date_max'] - new_merchant_trans_aggregated['new_purchase_date_min']).dt.days\n",
    "    new_merchant_trans_aggregated['new_purchase_date_uptomax'] = (datetime.datetime.today() - new_merchant_trans_aggregated['new_purchase_date_max']).dt.days\n",
    "    new_merchant_trans_aggregated['new_purchase_date_uptomin'] = (datetime.datetime.today() - new_merchant_trans_aggregated['new_purchase_date_min']).dt.days\n",
    "    \n",
    "    gc.collect\n",
    "    return new_merchant_trans_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_featurization(train):\n",
    "    print('loading test data....')\n",
    "    test = pd.read_csv('test.csv')\n",
    "    print('loading historical data....')\n",
    "    historical_transactions = reduce_memory(pd.read_csv('historical_transactions.csv'))\n",
    "    print('loading new merchants data....')\n",
    "    new_merchant_trans = reduce_memory(pd.read_csv('new_merchant_transactions.csv'))\n",
    "    print('Featurizing train and test....')\n",
    "    train_feat, test_feat = train_featurization(train, test)\n",
    "    print('Featurizing historical transactions....')\n",
    "    hist_aggregated = historical_transactions_featurization(historical_transactions)\n",
    "    print('Featurizing new merchant transactions....')\n",
    "    new_aggregated = new_merchants_featurization(new_merchant_trans)\n",
    "    print('Merging data....')\n",
    "    # merging train and aggregated historical transactions\n",
    "    train_hist = pd.merge(train, hist_aggregated, how = 'left', on = 'card_id')\n",
    "    test_hist = pd.merge(test, hist_aggregated, how = 'left', on = 'card_id')\n",
    "    # merging again with aggregated new merchants data\n",
    "    train_trans = pd.merge(train_hist, new_aggregated, how = 'left', on = 'card_id')\n",
    "    test_trans = pd.merge(test_hist, new_aggregated, how = 'left', on = 'card_id')\n",
    "    print('Featurizing....')\n",
    "    train_trans['trans_purchase_date_max'] = pd.to_datetime(train_trans['trans_purchase_date_max'])\n",
    "    train_trans['trans_purchase_date_min'] = pd.to_datetime(train_trans['trans_purchase_date_min'])\n",
    "    train_trans['new_purchase_date_max'] = pd.to_datetime(train_trans['new_purchase_date_max'])\n",
    "    train_trans['new_purchase_date_min'] = pd.to_datetime(train_trans['new_purchase_date_min'])\n",
    "\n",
    "    test_trans['trans_purchase_date_max'] = pd.to_datetime(test_trans['trans_purchase_date_max'])\n",
    "    test_trans['trans_purchase_date_min'] = pd.to_datetime(test_trans['trans_purchase_date_min'])\n",
    "    test_trans['new_purchase_date_max'] = pd.to_datetime(test_trans['new_purchase_date_max'])\n",
    "    test_trans['new_purchase_date_min'] = pd.to_datetime(test_trans['new_purchase_date_min'])\n",
    "    \n",
    "    #https://www.kaggle.com/chauhuynh/my-first-kernel-3-699\n",
    "    train_trans['trans_first_buy'] = train_trans['trans_purchase_date_min'] - train_trans['first_active_month']\n",
    "    train_trans['new_first_buy'] = train_trans['new_purchase_date_min'] - train_trans['first_active_month']\n",
    "\n",
    "    train_trans['card_id_total'] = train_trans['trans_card_id_size'] + train_trans['new_card_id_size']\n",
    "    train_trans['card_id_ratio'] = train_trans['trans_card_id_size'] / train_trans['new_card_id_size']\n",
    "\n",
    "    train_trans['purchase_amount_total'] = train_trans['trans_purchase_amount_sum'] + train_trans['new_purchase_amount_sum']\n",
    "    train_trans['purchase_amount_mean'] = train_trans['trans_purchase_amount_mean'] + train_trans['new_purchase_amount_mean']\n",
    "    train_trans['purchase_amount_var'] = train_trans['trans_purchase_amount_var'] + train_trans['new_purchase_amount_var']\n",
    "    train_trans['purchase_amount_max'] = train_trans['trans_purchase_amount_max'] + train_trans['new_purchase_amount_max']\n",
    "    train_trans['purchase_amount_min'] = train_trans['trans_purchase_amount_min'] + train_trans['new_purchase_amount_min']\n",
    "    train_trans['purchase_amount_skew'] = train_trans['trans_purchase_amount_skew'] + train_trans['new_purchase_amount_skew']\n",
    "\n",
    "    train_trans['installments_total'] = train_trans['trans_installments_sum'] + train_trans['new_installments_sum']\n",
    "    train_trans['installments_max'] = train_trans['trans_installments_max'] + train_trans['new_installments_max']\n",
    "    train_trans['installments_min'] = train_trans['trans_installments_min'] + train_trans['new_installments_min']\n",
    "    train_trans['installments_mean'] = train_trans['trans_installments_mean'] + train_trans['new_installments_mean']\n",
    "    train_trans['installments_var'] = train_trans['trans_installments_var'] + train_trans['new_installments_var']\n",
    "    train_trans['installments_skew'] = train_trans['trans_installments_skew'] + train_trans['new_installments_skew']\n",
    "\n",
    "    train_trans['month_lag_total'] = train_trans['trans_month_lag_sum'] + train_trans['new_month_lag_sum']\n",
    "    train_trans['month_lag_max'] = train_trans['trans_month_lag_max'] + train_trans['new_month_lag_max']\n",
    "    train_trans['month_lag_min'] = train_trans['trans_month_lag_min'] + train_trans['new_month_lag_min']\n",
    "    train_trans['month_lag_mean'] = train_trans['trans_month_lag_mean'] + train_trans['new_month_lag_mean']\n",
    "    train_trans['month_lag_var'] = train_trans['trans_month_lag_var'] + train_trans['new_month_lag_var']\n",
    "    train_trans['month_lag_skew'] = train_trans['trans_month_lag_skew'] + train_trans['new_month_lag_skew']\n",
    "\n",
    "    train_trans['month_diff_max'] = train_trans['trans_month_difference_max'] + train_trans['new_month_difference_max']\n",
    "    train_trans['month_diff_min'] = train_trans['trans_month_difference_min'] + train_trans['new_month_difference_min']\n",
    "    train_trans['month_diff_mean'] = train_trans['trans_month_difference_mean'] + train_trans['new_month_difference_mean']\n",
    "    train_trans['month_diff_var'] = train_trans['trans_month_difference_var'] + train_trans['new_month_difference_var']\n",
    "    train_trans['month_diff_skew'] = train_trans['trans_month_difference_skew'] + train_trans['new_month_difference_skew']\n",
    "    \n",
    "    test_trans['trans_first_buy'] = test_trans['trans_purchase_date_min'] - test_trans['first_active_month']\n",
    "    test_trans['new_first_buy'] = test_trans['new_purchase_date_min'] - test_trans['first_active_month']\n",
    "\n",
    "    test_trans['card_id_total'] = test_trans['trans_card_id_size'] + test_trans['new_card_id_size']\n",
    "    test_trans['card_id_ratio'] = test_trans['trans_card_id_size'] / test_trans['new_card_id_size']\n",
    "\n",
    "    test_trans['purchase_amount_total'] = test_trans['trans_purchase_amount_sum'] + test_trans['new_purchase_amount_sum']\n",
    "    test_trans['purchase_amount_mean'] = test_trans['trans_purchase_amount_mean'] + test_trans['new_purchase_amount_mean']\n",
    "    test_trans['purchase_amount_var'] = test_trans['trans_purchase_amount_var'] + test_trans['new_purchase_amount_var']\n",
    "    test_trans['purchase_amount_max'] = test_trans['trans_purchase_amount_max'] + test_trans['new_purchase_amount_max']\n",
    "    test_trans['purchase_amount_min'] = test_trans['trans_purchase_amount_min'] + test_trans['new_purchase_amount_min']\n",
    "    test_trans['purchase_amount_skew'] = test_trans['trans_purchase_amount_skew'] + test_trans['new_purchase_amount_skew']\n",
    "\n",
    "    test_trans['installments_total'] = test_trans['trans_installments_sum'] + test_trans['new_installments_sum']\n",
    "    test_trans['installments_max'] = test_trans['trans_installments_max'] + test_trans['new_installments_max']\n",
    "    test_trans['installments_min'] = test_trans['trans_installments_min'] + test_trans['new_installments_min']\n",
    "    test_trans['installments_mean'] = test_trans['trans_installments_mean'] + test_trans['new_installments_mean']\n",
    "    test_trans['installments_var'] = test_trans['trans_installments_var'] + test_trans['new_installments_var']\n",
    "    test_trans['installments_skew'] = test_trans['trans_installments_skew'] + test_trans['new_installments_skew']\n",
    "\n",
    "    test_trans['month_lag_total'] = test_trans['trans_month_lag_sum'] + test_trans['new_month_lag_sum']\n",
    "    test_trans['month_lag_max'] = test_trans['trans_month_lag_max'] + test_trans['new_month_lag_max']\n",
    "    test_trans['month_lag_min'] = test_trans['trans_month_lag_min'] + test_trans['new_month_lag_min']\n",
    "    test_trans['month_lag_mean'] = test_trans['trans_month_lag_mean'] + test_trans['new_month_lag_mean']\n",
    "    test_trans['month_lag_var'] = test_trans['trans_month_lag_var'] + test_trans['new_month_lag_var']\n",
    "    test_trans['month_lag_skew'] = test_trans['trans_month_lag_skew'] + test_trans['new_month_lag_skew']\n",
    "\n",
    "    test_trans['month_diff_max'] = test_trans['trans_month_difference_max'] + test_trans['new_month_difference_max']\n",
    "    test_trans['month_diff_min'] = test_trans['trans_month_difference_min'] + test_trans['new_month_difference_min']\n",
    "    test_trans['month_diff_mean'] = test_trans['trans_month_difference_mean'] + test_trans['new_month_difference_mean']\n",
    "    test_trans['month_diff_var'] = test_trans['trans_month_difference_var'] + test_trans['new_month_difference_var']\n",
    "    test_trans['month_diff_skew'] = test_trans['trans_month_difference_skew'] + test_trans['new_month_difference_skew']\n",
    "    # replacing inf values\n",
    "    train_trans.replace([-np.inf, np.inf], np.nan, inplace = True)\n",
    "    test_trans.replace([-np.inf, np.inf], np.nan, inplace = True)\n",
    "    # imputing missing values with mode\n",
    "    train_na = train_trans.columns[train_trans.isna().any()]\n",
    "    test_na = test_trans.columns[test_trans.isna().any()]\n",
    "    for i in range(len(train_na)):\n",
    "        train_trans[train_na[i]].fillna(train_trans[train_na[i]].mode()[0], inplace = True)\n",
    "        test_trans[train_na[i]].fillna(test_trans[train_na[i]].mode()[0], inplace = True)\n",
    "    s = train_trans.select_dtypes(include = ['datetime64[ns]']).columns\n",
    "    train_trans = train_trans.drop(s, axis = 1)\n",
    "    test_trans = test_trans.drop(s, axis = 1)\n",
    "    ns = train_trans.select_dtypes(include = ['timedelta64[ns]']).columns\n",
    "    for n in ns:\n",
    "        train_trans[n] = train_trans[n].astype(int)\n",
    "        test_trans[n] = test_trans[n].astype(int)\n",
    "    cols = [col for col in train_trans.columns if col not in ['card_id', 'outliers', 'target']]\n",
    "    return train_trans[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d1f970988176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'preprocessing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_featurization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('preprocessing', final_featurization(da)), ('model', pd.read_pickle('best_model.pkl'))])\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_function(train):\n",
    "    features = final_featurization(train)\n",
    "    model = pd.read_pickle('best_model.pkl')\n",
    "    predictions = model.predict(features)\n",
    "    print('Predicted loyalty score is : ', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading test data....\n",
      "loading historical data....\n",
      "Memory usage reduced to 1622.97 Mb(47.8% reduction)\n",
      "loading new merchants data....\n",
      "Memory usage reduced to 169.08 Mb(19.4% reduction)\n",
      "Featurizing train and test....\n",
      "Featurizing historical transactions....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage reduced to 1524.21 Mb(73.8% reduction)\n",
      "Featurizing new merchant transactions....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/series.py:2219: RuntimeWarning: overflow encountered in multiply\n",
      "  result = self._values.round(decimals)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage reduced to 121.29 Mb(70.0% reduction)\n",
      "Merging data....\n",
      "Featurizing....\n",
      "Predicted loyalty score is :  [-0.31623899 -0.71026829  0.50407203 ... -0.42776232 -2.03740717\n",
      " -0.47288799]\n",
      "rmse is :  3.4913335284219307\n"
     ]
    }
   ],
   "source": [
    "predict_function(train, train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
